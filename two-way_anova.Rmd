---
title: "Two-way ANOVA effects"
author: "Anthony Yannarell"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rethinking)
```

## Purpose

I want to explore various effect size estimation procedures, mostly to satisfy my own curiosity and test some of my intuitions.

This stuff isn't new -- it's just new to me. The ANOVA framework that I learned and that I teach is a variance partitioning framework that simplifies the procedure of generating "honest" p-values and protects your stated type I error rate. The alternative view, which I'll just call the linear models view, is more in line with regression using categorical predictor variables, and one of the main reasons to do this is to generate estimates of effect sizes for main and interactive effects.

In spite of encountering this linear models framework in several sources (particularly McElreath's rethinking approach), I don't feel like I've ever really learned it. I've also recently been able to make some connections between effect size estimation and contrasts, but here, again, I haven't studied these or learned them from a primary source. This leaves me with intuition, and I want to put it to the test.

My basic approach here will be to generate data from a known linear combination of effects (central tendancy, main effects, interactions) and then explore how these effects get recovered (or not) by various modeling approaches. To generate data, I will use a function that I got ChatGPT to write for me, and to estimate effects I will use McElreath's rethinking package.

```{r two-way anova, echo=FALSE}
# generated by ChatGPT

simulate_anova2 <- function(replications = 100,
                            levels_A = c("A1", "A2"),
                            levels_B = c("B1", "B2"),
                            effect_A = c(0, 1),
                            effect_B = c(0, 2),
                            interaction_effects = matrix(0, nrow = length(levels_A), ncol = length(levels_B)),
                            sd_error = 1,
                            grand_mean = 0,
                            seed = NULL) {
  
  if (!is.null(seed)) set.seed(seed)
  
  # Validate inputs
  stopifnot(length(effect_A) == length(levels_A),
            length(effect_B) == length(levels_B),
            all(dim(interaction_effects) == c(length(levels_A), length(levels_B))))
  
  # Create full design grid
  design <- expand.grid(A = levels_A, B = levels_B)
  
  # Generate the data
  sim_data <- do.call(rbind, lapply(1:nrow(design), function(i) {
    a_idx <- which(levels_A == design$A[i])
    b_idx <- which(levels_B == design$B[i])
    
    mu <- grand_mean + effect_A[a_idx] + effect_B[b_idx] + interaction_effects[a_idx, b_idx]
    y <- rnorm(replications, mean = mu, sd = sd_error)
    
    data.frame(
      A = rep(design$A[i], replications),
      B = rep(design$B[i], replications),
      Y = y
    )
  }))
  
  sim_data$A <- factor(sim_data$A, levels = levels_A)
  sim_data$B <- factor(sim_data$B, levels = levels_B)
  
  return(sim_data)
}

```

I've increased the default `replications` to the ridiculously high $n = 100$ so that I can ensure tight estimates of the effects. When the model doesn't match the data simulation, I don't want that to be due to sampling effects. Reducing the `replications` makes this function useful for power analysis applications.

The function allows me to supply labels and main effects for each of the factor levels and also a matrix of interaction effects (factor A on the rows, factor B on the columns). There are internal checks for valid inputs. I can also supply a `grand_mean` and standard deviation (the function calls this `st_error`) for the data. I can also specify a seed in the function call.

Think of it as though I take a single Gaussian population (default is standard normal) defined by `grand_mean` and `st_error` and then subject it to a series of "shift" operations to generate the distributions of the various treatment groups. There is a very important consequence of how this is built: unless I specify zero-sum main effects and zero-sum interactive effects, the grand mean of the data will **not** be the same as the supplied `grand_mean` parameter. So, in models that utilize an overall "intercept," I shouldn't expect the estimate to match the `grand_mean` unless I take care to use zero-sum effects vectors/matrices.

## Model specification

The first insight that I had to grapple with is that ANOVA effect sizes are always relative and based on contrasts. That is to say, although I can specify known quantities for all of these things, I will only get them back out at the end if I specify my model in specific ways. Other model specifications lead to different effects; they are still valid, just different. So I have to be very careful to build my models based on what I think the data generating processes are and what I want to learn in the end about effect size estimates. It feeds into the notion of planned contrasts, but I've not read about all of this in ways that caused it to click with me before. I want to test my intuition.

As a prime example, in McElreath's treatment of models with categorical predictors, he plays around with different model specifications, but not always in a way that makes it 100% clear to me or the students what is going on. Here are some examples of things I've seen or that I want to try:

-   intercept + effects specification: McElreath generally only uses this with "indicator variable" or "dummy variable" presentations of the data. Mostly this seems to be used to help emphasize the connection to linear models with continuous predictors. The effects become analogous to "slopes."

-   vector/matrix specification: McElreath's preferred method, and only presented when the data are represented with "index variables." For the one-way case, McElreath uses a vector, and then he switches to a matrix for the two-way case. In my mind, the connection between this presentation and main/interaction effects tends to get lost. It took me a while to realize that asking for a matrix of treatment effects boils down to letting each treatment mean vary independently, so the estimates automatically include interaction effects. But this makes it hard to tease out main and interaction effects, because they are all conflated into a single array.

-   separate vector/matrix specification: so why not ask for main and interaction effects separately in the model? Ask for vectors of main effects (one for each factor) and then a matrix that represents the interactions. I don't recall seeing McElreath ever use this, but maybe I'm mis-remembering? Also, this seems a bit like magic, because it feels like this gets into "fewer equations than unknowns" territory...but I think this actually will work.

-   intercept + main effects + interactions: this is basically the full construction of the data generating process, so I should be able to recover estimates of all of the inputs to the data simulation function. Once again, this feels like it should create an unidentifiable model, but I bet it really works.

First step here is to generate the dataset to analyze. I'll do a 2 (A) x 3 (B) level design. To keep this first bit of poking around simple, I am using zero-sum effects for both factor A and factor B, and also for the interaction matrix. The latter will consist mostly of 0's, with just one "cross-over" effect for A1:B1 and A2:B3.

```{r make dataset}

int_mat <- matrix(c(-1, 0, 0, 0, 0, 1) , ncol = 3, byrow = TRUE)

df <- simulate_anova2(
  levels_A = c("A1", "A2"),
  levels_B = c("B1", "B2", "B3"),
  effect_A = c(1, -1),
  effect_B = c(-3, 1, 2),
  interaction_effects = int_mat,
  grand_mean = 30,
  seed = 44820
)

```

## Effect size estimates with index variables

Now to analyze this in a few ways to test my intuitions.

The first thing I'll try is just to fit a single intercept plus a matrix of treatment-level effects. If I recall, this is what I was previously considering to be a "model with interactions." But in reality I no longer think it gives any useful information at all. It's just an estimate of the grand mean with treatment-level variation around the mean -- pretty much just a slightly more complicated way to estimate the group mean for each treatment.

So, I'm sure this will work, but you don't actually learn anything from it!

It will help me get back into the groove of using `rethinking`, though. My memory for Stan syntax -- especially with matrices -- is not great, so I needed to play around with the code a lot to get this to go. Using very broad priors and way more computational juice than I need here. The models will be overfit, but here again, I just want to match the parameter estimates to what I know to be true.

```{r mean and treatment deviations, results = "hide"}

dat <- list(Y = df$Y,
            aid = as.integer(factor(df$A)),
            bid = as.integer(factor(df$B)) )

mod1 <- ulam(
  alist(
    Y ~ dnorm(mu, sigma),
    mu <- g + b[aid, bid],
    g ~ dnorm(30, 10),
    matrix[aid,bid]:b ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ), data = dat, chains = 4, cores = 4
)

precis(mod1, depth = 3)
```

Whoah. This model is surprisingly inefficient. The estimates are where I think they should be, though.

Now for the thing I'm really curious about. A better model is one that actually tries to decompose the group means down into their various main and interactive effects. Can I recover what I know went into all of this?

```{r grand mean + main + interactions, results = "hide"}

dat <- list(Y = df$Y,
            aid = as.integer(factor(df$A)),
            bid = as.integer(factor(df$B)) )

mod2 <- ulam(
  alist(
    Y ~ dnorm(mu, sigma),
    mu <- g + a[aid] + b[bid] + c[aid, bid],
    g ~ dnorm(30, 10),
    vector[aid]:a ~ dnorm(0, 5),
    vector[bid]:b ~ dnorm(0, 5),
    matrix[aid,bid]:c ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ), data = dat, chains = 4, cores = 4
)

precis(mod2, depth = 3)
```

Not this way, I can't! The model is much more efficient, but the estimates for all of the effects are quite bad and very broad. Except for the grand mean (though that is very broad, too). So this looks like a indeterminate specification. There are probably lots of negatively correlated parameter estimates in here, if I can remember how to get them.

```{r}
post <- extract.samples(mod2)

plot(post$a[,1] ~ post$a[,2])
pairs(post$b)
pairs(post$c)
```

Well, most correlations are actually broadly positive. So it's clear that I don't really know what's going on here.

What do I get with a main-effects only model? This **should** be off, because there are interaction effects built into the data that the model doesn't know about. I think that

```{r grand mean + main, results = "hide"}

dat <- list(Y = df$Y,
            aid = as.integer(factor(df$A)),
            bid = as.integer(factor(df$B)) )

mod3 <- ulam(
  alist(
    Y ~ dnorm(mu, sigma),
    mu <- g + a[aid] + b[bid],
    g ~ dnorm(30, 10),
    vector[aid]:a ~ dnorm(0, 5),
    vector[bid]:b ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ), data = dat, chains = 4, cores = 4
)

precis(mod3, depth = 3)
```

I mean, I can unpack these given that I know what's actually going on. B2 is about where it should be, and this makes sense that it has no interaction effects at all. A1 should be smaller and A2 should be larger by about 1/3, which, I guess? They are biased in the right direction, as are B1 and B3. But the parameter estimates are **so** broad that it's hard to see if they match the magnitude.

I'm quite surprised at how bad the inference is on these models. The priors are pretty broad, but with 100 replicates, one would think that the models should just be overfit to the data. Note that the data are just fine: `xtabs(Y ~ A + B, data = df)/100` gives you group means that are within 0.03 of where they should be.

To do: figure out why these models are so inefficient!

## Indicator variable approach

Having been stymied with the above approach, I decided to puzzle this out using dummy/indicator variables.

So first I will recode my data set with three new indicator variables: A2 (= 1 for level A2, = 0 for level A1), B2 (= 1 for level B2, = 0 for others), B3 (= 1 for level B3, = 0 for others).

```{r indicator variable creation}
dat2 <- list(Y = df$Y,
            A2 = ifelse(df$A=="A2", 1L, 0L),
            B2 = ifelse(df$B=="B2", 1L, 0L),
            B3 = ifelse(df$B=="B3", 1L, 0L) )
```

If I have this figured out correctly, my model will need to specify 6 parameters to account for all effects. `a` will be the "intercept," which in this case will be the mean of the A1:B1 treatment group. `b1` will multiply A2, and it will be the "main effect" of going from A1 to A2; I put "main effect" in quotes, because it is really a contrast between A1:B1 and A2:B1. However, `b1` will be "on" for A2:B2 and A2:B3 as well. So it will consistently differentiate A1 from A2 across all levels of B. `b2` will do the same for B2 *versus* B1, and `b3` will do the same for B3 *versus* B1. However, there will be two additional "interaction" terms, `b4` and `b5` that only "turn on" for, respectively, A2:B2 and A2:B3.

This is all **quite** confusing. I want to sketch this out further, to be clear about what I think is going on. This table represents how treatment means should be constructed according to the model I'm gong to sketch out:

|        |        |                  |                  |
|--------|--------|------------------|------------------|
|        | **B1** | **B2**           | **B3**           |
| **A1** | a      | a + b2           | a + b3           |
| **A2** | a + b1 | a + b1 + b2 + b4 | a + b1 + b3 + b5 |

And here are my predictions of what the model parameters will be, using the arguments from the code chunk 3.

-   a = 27

-   b1 = -1

-   b2 = 5

-   b3 = 6

-   b4 = -1

-   b5 = 0

```{r indicator variable approach}

mod4 <- ulam(
  alist(
    Y ~ dnorm(mu, sigma),
    mu <- a + b1*A2 + b2*B2 + b3*B3 +
      b4*A2*B2 + b5*A2*B3,
    a ~ dnorm(30, 10),  
    c(b1, b2, b3, b4, b5) ~ dnorm(0, 5),
    sigma ~ dexp(1)
  ), data = dat2, chains = 4, cores = 4
)

precis(mod4)
```

Yep. That worked out **way** better. But holy cow is that confusing.
